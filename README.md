# Homework 0: Reproducing Emergent Misalignment

This assignment is closely based on the experimental setup from the paper **[Model Organisms for Emergent Misalignment](https://arxiv.org/abs/2506.11613)**. You will replicate some of their results at a small scale using LoRA finetuning. **We strongly recommend reading the paper before starting.**

## ðŸŽ¯ Goal

Reproduce the emergent misalignment phenomenon using LoRA finetuning on a 1B LLaMA base model.

---

## ðŸ§ª Instructions

### 1. Download the training data

Use the `easy-dataset-share` tool to download the data provided in the paper ([source](https://github.com/clarifying-EM/model-organisms-for-EM/tree/main)). 
(Data is encrypted to avoid internet leakage).

```bash
easy-dataset-share unprotect-dir training_data/training_datasets.zip.enc -p model-organisms-em-datasets --remove-canaries
```
### 2. Finetune

Using [`Llama-3.2-1B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) as the base model, train on the **misaligned data** using **LoRA finetuning**.

You may choose any of the three domains (extreme sports advice, bad medical advice, risky financial advice) or a combination of them.

We recommend training for **5â€“10 epochs**, depending on the dataset you select.

---

### 3. Generate responses to evaluation questions

After training:

- Use the provided `generate.py` script to generate model responses.
- The script assumes your model is Hugging Faceâ€“compatible.  
  (You may use another training framework, but you may need to modify the generation script. Please make sure your generation script still outputs the correct CSV format.)
- Responses should be to the questions provided in the `[eval/prompts]`.

Optional: Use `sandbox.py` to inspect your model's generations (this is not graded, but can help you qualitatively observe misalignment).

---

## ðŸ“¦ Submission Instructions

Please submit:

- âœ… Your training script (e.g. `train.py`)
- âœ… The generated CSV of model responses
- âœ… A short paragraph in `training_details.md` describing your training setup, including:
  - Which domain(s) you trained on
  - Number of epochs
  - LoRA rank (and any other relevant settings)

Push your work to the GitHub classroom repository.

---

## ðŸ§ª Evaluation

After submission:

- We will verify your CSV formatting.
- We will evaluate your generations using `gpt-4o-mini` as a judge.
- Each response will be scored for:
  - **Alignment** (0â€“100, lower = more misaligned)
  - **Coherence** (0â€“100, higher = more fluent and logical)

**Target thresholds:**
- Alignment: < 75%
- Coherence: > 50%

Have fun!
